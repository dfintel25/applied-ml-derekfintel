{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Data Analysis\n",
    "**Author:** Derek Fintel\n",
    "\n",
    "**Date:** April, 04th, 2025 \n",
    "\n",
    "**Objective:** Predicting a Continuous Target with Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this project we utilize a trusted Titanic dataset to conduct various analyses, exercise functions, and provide meaningful predicitions of target data. \n",
    "\n",
    "This project is organized into the following Sections:\n",
    "- Section 0: Imports\n",
    "- Section 1: Load and Inspect the Data\n",
    "- Section 2: Data Exploration and Preparation\n",
    "- Section 3: Feature Selection and Justification\n",
    "- Section 4: Train a Regression Model (Linear Regression)\n",
    "- Section 5: Compare Alternative Models\n",
    "- Section 6: Final Thoughts & Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports  \n",
    "Below are our modules used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import pandas for data manipulation and analysis. \n",
    "import pandas as pd\n",
    "\n",
    "# Import pandas for data manipulation and analysis.\n",
    "import numpy as np\n",
    "\n",
    "# Import matplotlib for creating static visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import seaborn for statistical data visualization (built on matplotlib)\n",
    "import seaborn as sns\n",
    "\n",
    "# Import the California housing dataset from sklearn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Import train_test_split for splitting data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import LinearRegression for building a linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Import performance metrics for model evaluation\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Import Pandas Plotting and Scatter Matrix\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# Import Stratified Shuffle Split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Load and Inspect the Data\n",
    "\n",
    "### 1.1 Load the dataset and display its info\n",
    "- We load the Titanic dataset directly from `seaborn`.\n",
    "- We display summary information of the dataset using the info() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   survived     891 non-null    int64   \n",
      " 1   pclass       891 non-null    int64   \n",
      " 2   sex          891 non-null    object  \n",
      " 3   age          714 non-null    float64 \n",
      " 4   sibsp        891 non-null    int64   \n",
      " 5   parch        891 non-null    int64   \n",
      " 6   fare         891 non-null    float64 \n",
      " 7   embarked     889 non-null    object  \n",
      " 8   class        891 non-null    category\n",
      " 9   who          891 non-null    object  \n",
      " 10  adult_male   891 non-null    bool    \n",
      " 11  deck         203 non-null    category\n",
      " 12  embark_town  889 non-null    object  \n",
      " 13  alive        891 non-null    object  \n",
      " 14  alone        891 non-null    bool    \n",
      "dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n",
      "memory usage: 80.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# We Load the 'titantic' dataset via sns.load_dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "#We retrieve its summary info via '.info()'\n",
    "titanic.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2218892b",
   "metadata": {},
   "source": [
    "### 1.2 Display the first 10 rows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "id": "16ce92fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   survived  pclass     sex   age  sibsp  parch     fare embarked   class  \\\n",
      "0         0       3    male  22.0      1      0   7.2500        S   Third   \n",
      "1         1       1  female  38.0      1      0  71.2833        C   First   \n",
      "2         1       3  female  26.0      0      0   7.9250        S   Third   \n",
      "3         1       1  female  35.0      1      0  53.1000        S   First   \n",
      "4         0       3    male  35.0      0      0   8.0500        S   Third   \n",
      "5         0       3    male   NaN      0      0   8.4583        Q   Third   \n",
      "6         0       1    male  54.0      0      0  51.8625        S   First   \n",
      "7         0       3    male   2.0      3      1  21.0750        S   Third   \n",
      "8         1       3  female  27.0      0      2  11.1333        S   Third   \n",
      "9         1       2  female  14.0      1      0  30.0708        C  Second   \n",
      "\n",
      "     who  adult_male deck  embark_town alive  alone  \n",
      "0    man        True  NaN  Southampton    no  False  \n",
      "1  woman       False    C    Cherbourg   yes  False  \n",
      "2  woman       False  NaN  Southampton   yes   True  \n",
      "3  woman       False    C  Southampton   yes  False  \n",
      "4    man        True  NaN  Southampton    no   True  \n",
      "5    man        True  NaN   Queenstown    no   True  \n",
      "6    man        True    E  Southampton    no   True  \n",
      "7  child       False  NaN  Southampton    no  False  \n",
      "8  woman       False  NaN  Southampton   yes  False  \n",
      "9  child       False  NaN    Cherbourg   yes  False  \n"
     ]
    }
   ],
   "source": [
    "# Here we 'print' the first 10 rows via '.head(10)'\n",
    "print(titanic.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Data Exploration and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc9e869",
   "metadata": {},
   "source": [
    "### 2.1 Explore Data Patterns and Distributions\n",
    "Prepare the Titanic data for regression modeling. See the previous work.\n",
    "\n",
    "Impute missing values for age using median\n",
    "Drop rows with missing fare (or impute if preferred)\n",
    "Create numeric variables (e.g., family_size from sibsp + parch + 1)\n",
    "Optional - convert categorical features (e.g. sex, embarked) if you think they might help your prediction model. (We do not know relationships until we evaluate things.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derek\\AppData\\Local\\Temp\\ipykernel_3716\\1354486036.py:1: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
    "\n",
    "titanic = titanic.dropna(subset=['fare'])\n",
    "\n",
    "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. Feature Selection and Justification\n",
    "### 3.1 Choose two input features for predicting the target\n",
    "Define multiple combinations of features to use as inputs to predict fare.\n",
    "\n",
    "Use unique names (X1, y1, X2, y2, etc.) so results are visible and can be compared at the same time. \n",
    "\n",
    "Remember the inputs, usually X, are a 2D array. The target is a 1D array. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d022a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1. age\n",
    "X1 = titanic[['age']]\n",
    "y1 = titanic['fare']\n",
    "\n",
    "# Case 2. family_size\n",
    "X2 = titanic[['family_size']]\n",
    "y2 = titanic['fare']\n",
    "\n",
    "# Case 3. age, family_size\n",
    "X3 = titanic[['age', 'family_size']]\n",
    "y3 = titanic['fare']\n",
    "\n",
    "# Case 4. ???\n",
    "X4 = titanic[['parch']]\n",
    "y4 = titanic['fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ee629",
   "metadata": {},
   "source": [
    "### Reflection of Section 3:\n",
    "\n",
    "1) Why might these features affect a passenger’s fare:\n",
    "2) List all available features:\n",
    "3) Which other features could improve predictions and why:\n",
    "4) How many variables are in your Case 4:\n",
    "5) Which variable(s) did you choose for Case 4 and why do you feel those could make good inputs: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4. Train a Regression Model (Linear Regression)\n",
    "\n",
    "### 4.1 Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "id": "e1b27c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=123)\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=123)\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=123)\n",
    "\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca86eb",
   "metadata": {},
   "source": [
    "### 4.2 Train and Evaluate Linear Regression Models (all 4 cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "id": "23199b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model1 = LinearRegression().fit(X1_train, y1_train)\n",
    "lr_model2 = LinearRegression().fit(X2_train, y2_train)\n",
    "lr_model3 = LinearRegression().fit(X3_train, y3_train)\n",
    "lr_model4 = LinearRegression().fit(X4_train, y4_train)\n",
    "\n",
    "# Predictions\n",
    "\n",
    "y_pred_train1 = lr_model1.predict(X1_train)\n",
    "y_pred_test1 = lr_model1.predict(X1_test)\n",
    "\n",
    "y_pred_train2 = lr_model2.predict(X2_train)\n",
    "y_pred_test2 = lr_model2.predict(X2_test)\n",
    "\n",
    "# TODO: repeat for case 3 and 4 .... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85af3d24",
   "metadata": {},
   "source": [
    "### 4.3 Report Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "id": "844440cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y1_pred_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[99]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCase 1: Training R²:\u001b[39m\u001b[33m\"\u001b[39m, r2_score(y1_train, \u001b[43my1_pred_train\u001b[49m))\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCase 1: Test R²:\u001b[39m\u001b[33m\"\u001b[39m, r2_score(y1_test, y1_pred_test))\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCase 1: Test RMSE:\u001b[39m\u001b[33m\"\u001b[39m, mean_squared_error(y1_test, y1_pred_test, squared=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[31mNameError\u001b[39m: name 'y1_pred_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Case 1: Training R²:\", r2_score(y1_train, y1_pred_train))\n",
    "print(\"Case 1: Test R²:\", r2_score(y1_test, y1_pred_test))\n",
    "print(\"Case 1: Test RMSE:\", mean_squared_error(y1_test, y1_pred_test, squared=False))\n",
    "print(\"Case 1: Test MAE:\", mean_absolute_error(y1_test, y1_pred_test))\n",
    "\n",
    "# TODO: Repeat for Cases 2-4...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection of Section 4:\n",
    "\n",
    "Compare the train vs test results for each.\n",
    "1) Did Case 1 overfit or underfit? Explain:\n",
    "2) Did Case 2 overfit or underfit? Explain:\n",
    "3) Did Case 3 overfit or underfit? Explain:\n",
    "4) Did Case 4 overfit or underfit? Explain:\n",
    "\n",
    "Adding Age\n",
    "1) Did adding age improve the model:\n",
    "2) Propose a possible explanation (consider how age might affect ticket price, and whether the data supports that): \n",
    "\n",
    "Worst\n",
    "1) Which case performed the worst:\n",
    "2) How do you know: \n",
    "3) Do you think adding more training data would improve it (and why/why not): \n",
    "\n",
    "Best\n",
    "1) Which case performed the best:\n",
    "2) How do you know: \n",
    "3) Do you think adding more training data would improve it (and why/why not): \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e48ce",
   "metadata": {},
   "source": [
    "### Section 5. Compare Alternative Models\n",
    "In this section, we will take the best-performing case and explore other regression models.\n",
    "\n",
    "Choose Best Case to Continue\n",
    "Choose the best case model from the four cases. Use that model to continue to explore additional continuous prediction models. The following assumes that Case 1 was the best predictor  - this may not be the case. Adjust the code to use your best case model instead. \n",
    "\n",
    "Choosing Options\n",
    "When working with regression models, especially those with multiple input features, we may run into overfitting — where a model fits the training data too closely and performs poorly on new data. To prevent this, we can apply regularization.\n",
    "\n",
    "Regularization adds a penalty to the model’s loss function, discouraging it from using very large weights (coefficients). This makes the model simpler and more likely to generalize well to new data.\n",
    "\n",
    "In general: \n",
    "- If the basic linear regression is overfitting, try Ridge.\n",
    "- If you want the model to automatically select the most important features, try Lasso.\n",
    "- If you want a balanced approach, try Elastic Net."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fb5dad",
   "metadata": {},
   "source": [
    "### 5.2 Elastic Net (L1 + L2 combined)\n",
    "Lasso Regression uses the L1 penalty, which adds the sum of absolute values of the coefficients to the loss function. Lasso can shrink some coefficients all the way to zero, effectively removing less important features. This makes it useful for feature selection.\n",
    "\n",
    "- Penalty term: L1 = sum of absolute values of weights\n",
    "- Effect: Can shrink some weights to zero (drops features), simplifies the model\n",
    "Elastic Net combines both L1 (Lasso) and L2 (Ridge) penalties. It balances the feature selection ability of Lasso with the stability of Ridge.\n",
    "\n",
    "We control the balance with a parameter called l1_ratio:\n",
    "- If l1_ratio = 0, it behaves like Ridge\n",
    "- If l1_ratio = 1, it behaves like Lasso\n",
    "- Values in between mix both types\n",
    "- Penalty term: α × (L1 + L2)\n",
    "- Effect: Shrinks weights and can drop some features — flexible and powerful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84943b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "elastic_model = ElasticNet(alpha=0.3, l1_ratio=0.5)\n",
    "elastic_model.fit(X1_train, y1_train)\n",
    "y_pred_elastic = elastic_model.predict(X1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e9eae3",
   "metadata": {},
   "source": [
    "### 5.3 Polynomial Regression\n",
    "Linear regression is a simple two dimensional relationship - a simple straight line. But we can test more complex relationships. Polynomial regression adds interaction and nonlinear terms to the model. Be careful here - higher-degree polynomials can easily overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce0b33",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set up the poly inputs\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_train_poly = poly.fit_transform(X1_train)\n",
    "X_test_poly = poly.transform(X1_test)\n",
    "\n",
    "# Use the poly inputs in the LR model\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_train_poly, y1_train)\n",
    "y_pred_poly = poly_model.predict(X1_test_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be2652",
   "metadata": {},
   "source": [
    "### 5.4 Visualize Polynomial Cubic Fit (for 1 input feature)\n",
    "Choose a case with just one input feature and plot it. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9beafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X1_test, y1_test, color='blue', label='Actual')\n",
    "plt.scatter(X1_test, y_pred_poly, color='red', label='Predicted (Poly)')\n",
    "plt.legend()\n",
    "plt.title(\"Polynomial Regression: Age vs Fare\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78a6aac",
   "metadata": {},
   "source": [
    "### 5.4 Reflections:\n",
    "\n",
    "1) What patterns does the cubic model seem to capture:\n",
    "2) Where does it perform well or poorly:\n",
    "3) Did the polynomial fit outperform linear regression:\n",
    "4) Where (on the graph or among which kinds of data points) does it fit best:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dca114",
   "metadata": {},
   "source": [
    "### 5.4 Compare All Models\n",
    "Create a summary table or printout comparing all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f504de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(name, y_true, y_pred):\n",
    "    print(f\"{name} R²: {r2_score(y_true, y_pred):.3f}\")\n",
    "    print(f\"{name} RMSE: {mean_squared_error(y_true, y_pred, squared=False):.2f}\")\n",
    "    print(f\"{name} MAE: {mean_absolute_error(y_true, y_pred):.2f}\\n\")\n",
    "\n",
    "report(\"Linear\", y1_test, y1_pred_test)\n",
    "report(\"Ridge\", y1_test, y_pred_ridge)\n",
    "report(\"ElasticNet\", y1_test, y_pred_elastic)\n",
    "report(\"Polynomial\", y1_test, y_pred_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341db63d",
   "metadata": {},
   "source": [
    "### 5.5 Visualize Higher Order Polynomial (for the same 1 input case)\n",
    "Use the same single input case as you visualized above, but use a higher degree polynomial (e.g. 4, 5, 6, 7, or 8). Plot the result. \n",
    "\n",
    "In a Markdown cell, tell us which option seems to work better - your initial cubic (3) or your higher order and why. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da5716d",
   "metadata": {},
   "source": [
    "### Section 6. Final Thoughts & Insights\n",
    "Your notebook should tell a data story. Use this section to demonstrate your thinking and value as an analyst.\n",
    "\n",
    "### 6.1 Summarize Findings\n",
    "1) What features were most useful?\n",
    "2) What regression model performed best?\n",
    "3) How did model complexity or regularization affect results?\n",
    "\n",
    "### 6.2 Discuss Challenges\n",
    "1) Was fare hard to predict? Why?\n",
    "2) Did skew or outliers impact the models?\n",
    "\n",
    "### 6.3 Optional Next Steps\n",
    "1) Try different features besides the ones used (e.g., pclass, sex if you didn't use them this time)\n",
    "2) Try predicting age instead of fare\n",
    "3) Explore log transformation of fare to reduce skew"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
