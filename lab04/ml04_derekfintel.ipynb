{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Titanic Data Analysis\n",
    "**Author:** Derek Fintel\n",
    "\n",
    "**Date:** April, 04th, 2025 \n",
    "\n",
    "**Objective:** Predicting a Continuous Target with Regression.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "In this project we utilize a trusted Titanic dataset to conduct various analyses, exercise functions, and provide meaningful predicitions of target data. \n",
    "\n",
    "This project is organized into the following Sections:\n",
    "- Section 0: Imports\n",
    "- Section 1: Load and Inspect the Data\n",
    "- Section 2: Data Exploration and Preparation\n",
    "- Section 3: Feature Selection and Justification\n",
    "- Section 4: Train a Regression Model (Linear Regression)\n",
    "- Section 5: Compare Alternative Models\n",
    "- Section 6: Final Thoughts & Insights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports  \n",
    "Below are our modules used."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Import pandas for data manipulation and analysis. \n",
    "import pandas as pd\n",
    "\n",
    "# Import pandas for data manipulation and analysis.\n",
    "import numpy as np\n",
    "\n",
    "# Import matplotlib for creating static visualizations\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import seaborn for statistical data visualization (built on matplotlib)\n",
    "import seaborn as sns\n",
    "\n",
    "# Import the California housing dataset from sklearn\n",
    "from sklearn.datasets import fetch_california_housing\n",
    "\n",
    "# Import train_test_split for splitting data into training and test sets\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Import LinearRegression for building a linear regression model\n",
    "from sklearn.linear_model import LinearRegression\n",
    "\n",
    "# Import performance metrics for model evaluation\n",
    "from sklearn.metrics import root_mean_squared_error, mean_absolute_error, r2_score\n",
    "\n",
    "# Import Pandas Plotting and Scatter Matrix\n",
    "from pandas.plotting import scatter_matrix\n",
    "\n",
    "# Import Stratified Shuffle Split\n",
    "from sklearn.model_selection import StratifiedShuffleSplit\n",
    "\n",
    "import seaborn as sns\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.linear_model import LinearRegression, Ridge, ElasticNet\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error, r2_score"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 1. Load and Inspect the Data\n",
    "\n",
    "### 1.1 Load the dataset and display its info\n",
    "- We load the Titanic dataset directly from `seaborn`.\n",
    "- We display summary information of the dataset using the info() method."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 891 entries, 0 to 890\n",
      "Data columns (total 15 columns):\n",
      " #   Column       Non-Null Count  Dtype   \n",
      "---  ------       --------------  -----   \n",
      " 0   survived     891 non-null    int64   \n",
      " 1   pclass       891 non-null    int64   \n",
      " 2   sex          891 non-null    object  \n",
      " 3   age          714 non-null    float64 \n",
      " 4   sibsp        891 non-null    int64   \n",
      " 5   parch        891 non-null    int64   \n",
      " 6   fare         891 non-null    float64 \n",
      " 7   embarked     889 non-null    object  \n",
      " 8   class        891 non-null    category\n",
      " 9   who          891 non-null    object  \n",
      " 10  adult_male   891 non-null    bool    \n",
      " 11  deck         203 non-null    category\n",
      " 12  embark_town  889 non-null    object  \n",
      " 13  alive        891 non-null    object  \n",
      " 14  alone        891 non-null    bool    \n",
      "dtypes: bool(2), category(2), float64(2), int64(4), object(5)\n",
      "memory usage: 80.7+ KB\n"
     ]
    }
   ],
   "source": [
    "# We Load the 'titantic' dataset via sns.load_dataset\n",
    "titanic = sns.load_dataset('titanic')\n",
    "\n",
    "#We retrieve its summary info via '.info()'\n",
    "titanic.info()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2218892b",
   "metadata": {},
   "source": [
    "### 1.2 Display the first 10 rows.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "id": "16ce92fc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   survived  pclass     sex   age  sibsp  parch     fare embarked   class  \\\n",
      "0         0       3    male  22.0      1      0   7.2500        S   Third   \n",
      "1         1       1  female  38.0      1      0  71.2833        C   First   \n",
      "2         1       3  female  26.0      0      0   7.9250        S   Third   \n",
      "3         1       1  female  35.0      1      0  53.1000        S   First   \n",
      "4         0       3    male  35.0      0      0   8.0500        S   Third   \n",
      "5         0       3    male   NaN      0      0   8.4583        Q   Third   \n",
      "6         0       1    male  54.0      0      0  51.8625        S   First   \n",
      "7         0       3    male   2.0      3      1  21.0750        S   Third   \n",
      "8         1       3  female  27.0      0      2  11.1333        S   Third   \n",
      "9         1       2  female  14.0      1      0  30.0708        C  Second   \n",
      "\n",
      "     who  adult_male deck  embark_town alive  alone  \n",
      "0    man        True  NaN  Southampton    no  False  \n",
      "1  woman       False    C    Cherbourg   yes  False  \n",
      "2  woman       False  NaN  Southampton   yes   True  \n",
      "3  woman       False    C  Southampton   yes  False  \n",
      "4    man        True  NaN  Southampton    no   True  \n",
      "5    man        True  NaN   Queenstown    no   True  \n",
      "6    man        True    E  Southampton    no   True  \n",
      "7  child       False  NaN  Southampton    no  False  \n",
      "8  woman       False  NaN  Southampton   yes  False  \n",
      "9  child       False  NaN    Cherbourg   yes  False  \n"
     ]
    }
   ],
   "source": [
    "# Here we 'print' the first 10 rows via '.head(10)'\n",
    "print(titanic.head(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 2. Data Exploration and Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1bc9e869",
   "metadata": {},
   "source": [
    "### 2.1 Explore Data Patterns and Distributions\n",
    "Prepare the Titanic data for regression modeling. See the previous work.\n",
    "\n",
    "1) Impute missing values for age using median\n",
    "2) Drop rows with missing fare (or impute if preferred)\n",
    "3) Create numeric variables (e.g., family_size from sibsp + parch + 1)\n",
    "4) Optional - convert categorical features (e.g. sex, embarked) if you think they might help your prediction model. (We do not know relationships until we evaluate things.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   survived  pclass  sex   age  sibsp  parch     fare  class    who  \\\n",
      "0         0       3    0  22.0      1      0   7.2500  Third    man   \n",
      "1         1       1    1  38.0      1      0  71.2833  First  woman   \n",
      "2         1       3    1  26.0      0      0   7.9250  Third  woman   \n",
      "3         1       1    1  35.0      1      0  53.1000  First  woman   \n",
      "4         0       3    0  35.0      0      0   8.0500  Third    man   \n",
      "\n",
      "   adult_male deck  embark_town alive  alone  family_size  embarked_Q  \\\n",
      "0        True  NaN  Southampton    no  False            2       False   \n",
      "1       False    C    Cherbourg   yes  False            2       False   \n",
      "2       False  NaN  Southampton   yes   True            1       False   \n",
      "3       False    C  Southampton   yes  False            2       False   \n",
      "4        True  NaN  Southampton    no   True            1       False   \n",
      "\n",
      "   embarked_S  \n",
      "0        True  \n",
      "1       False  \n",
      "2        True  \n",
      "3        True  \n",
      "4        True  \n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\derek\\AppData\\Local\\Temp\\ipykernel_3716\\468602673.py:8: FutureWarning: A value is trying to be set on a copy of a DataFrame or Series through chained assignment using an inplace method.\n",
      "The behavior will change in pandas 3.0. This inplace method will never work because the intermediate object on which we are setting values always behaves as a copy.\n",
      "\n",
      "For example, when doing 'df[col].method(value, inplace=True)', try using 'df.method({col: value}, inplace=True)' or df[col] = df[col].method(value) instead, to perform the operation inplace on the original object.\n",
      "\n",
      "\n",
      "  titanic['age'].fillna(titanic['age'].median(), inplace=True)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Load data (adjust path if needed)\n",
    "#titanic = pd.read_csv('titanic.csv')\n",
    "\n",
    "# 1. Impute missing 'Age' values using median\n",
    "titanic['age'].fillna(titanic['age'].median(), inplace=True)\n",
    "\n",
    "# 2. Drop rows with missing 'fare' (or impute if preferred)\n",
    "titanic.dropna(subset=['fare'], inplace=True)\n",
    "\n",
    "# Alternatively, you can impute fare instead of dropping:\n",
    "# titanic['fare'].fillna(titanic['fare'].median(), inplace=True)\n",
    "\n",
    "# 3. Create 'family_size' feature\n",
    "titanic['family_size'] = titanic['sibsp'] + titanic['parch'] + 1\n",
    "\n",
    "# 4. Optional: Convert categorical features to numeric\n",
    "\n",
    "# Convert 'sex' to binary\n",
    "titanic['sex'] = titanic['sex'].map({'male': 0, 'female': 1})\n",
    "\n",
    "# One-hot encode 'embarked' (drop_first avoids multicollinearity)\n",
    "titanic = pd.get_dummies(titanic, columns=['embarked'], drop_first=True)\n",
    "\n",
    "# Preview the cleaned data\n",
    "print(titanic.head())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 3. Feature Selection and Justification\n",
    "### 3.1 Choose two input features for predicting the target\n",
    "Define multiple combinations of features to use as inputs to predict fare.\n",
    "\n",
    "Use unique names (X1, y1, X2, y2, etc.) so results are visible and can be compared at the same time. \n",
    "\n",
    "Remember the inputs, usually X, are a 2D array. The target is a 1D array. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "id": "d022a37f",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Case 1. age\n",
    "X1 = titanic[['age']]\n",
    "y1 = titanic['fare']\n",
    "\n",
    "# Case 2. family_size\n",
    "X2 = titanic[['family_size']]\n",
    "y2 = titanic['fare']\n",
    "\n",
    "# Case 3. age, family_size\n",
    "X3 = titanic[['age', 'family_size']]\n",
    "y3 = titanic['fare']\n",
    "\n",
    "# Case 4. parch\n",
    "X4 = titanic[['parch']]\n",
    "y4 = titanic['fare']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "416ee629",
   "metadata": {},
   "source": [
    "### Reflection of Section 3:\n",
    "\n",
    "1) Why might these features affect a passenger’s fare:\n",
    "2) List all available features:\n",
    "3) Which other features could improve predictions and why:\n",
    "4) How many variables are in your Case 4:\n",
    "5) Which variable(s) did you choose for Case 4 and why do you feel those could make good inputs: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Section 4. Train a Regression Model (Linear Regression)\n",
    "\n",
    "### 4.1 Split the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "id": "ef42647e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Case 1: age\n",
      "Training R²: 0.009950688019452314\n",
      "Test R²: 0.0034163395508415295\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "got an unexpected keyword argument 'squared'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mTypeError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[126]\u001b[39m\u001b[32m, line 30\u001b[39m\n\u001b[32m     27\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest RMSE:\u001b[39m\u001b[33m\"\u001b[39m, mean_squared_error(y_test, y_pred_test, squared=\u001b[38;5;28;01mFalse\u001b[39;00m))\n\u001b[32m     28\u001b[39m     \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest MAE:\u001b[39m\u001b[33m\"\u001b[39m, mean_absolute_error(y_test, y_pred_test))\n\u001b[32m---> \u001b[39m\u001b[32m30\u001b[39m \u001b[43mprint_metrics\u001b[49m\u001b[43m(\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mCase 1: age\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_train1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my1_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_test1\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     31\u001b[39m print_metrics(\u001b[33m\"\u001b[39m\u001b[33mCase 2: family_size\u001b[39m\u001b[33m\"\u001b[39m, y2_train, y_pred_train2, y2_test, y_pred_test2)\n\u001b[32m     32\u001b[39m print_metrics(\u001b[33m\"\u001b[39m\u001b[33mCase 3: age + family_size\u001b[39m\u001b[33m\"\u001b[39m, y3_train, y_pred_train3, y3_test, y_pred_test3)\n",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[126]\u001b[39m\u001b[32m, line 27\u001b[39m, in \u001b[36mprint_metrics\u001b[39m\u001b[34m(case_name, y_train, y_pred_train, y_test, y_pred_test)\u001b[39m\n\u001b[32m     25\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTraining R²:\u001b[39m\u001b[33m\"\u001b[39m, r2_score(y_train, y_pred_train))\n\u001b[32m     26\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest R²:\u001b[39m\u001b[33m\"\u001b[39m, r2_score(y_test, y_pred_test))\n\u001b[32m---> \u001b[39m\u001b[32m27\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest RMSE:\u001b[39m\u001b[33m\"\u001b[39m, \u001b[43mmean_squared_error\u001b[49m\u001b[43m(\u001b[49m\u001b[43my_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_pred_test\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msquared\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mFalse\u001b[39;49;00m\u001b[43m)\u001b[49m)\n\u001b[32m     28\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mTest MAE:\u001b[39m\u001b[33m\"\u001b[39m, mean_absolute_error(y_test, y_pred_test))\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\applied-ml-derekfintel\\.venv\\Lib\\site-packages\\sklearn\\utils\\_param_validation.py:194\u001b[39m, in \u001b[36mvalidate_params.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(*args, **kwargs)\u001b[39m\n\u001b[32m    191\u001b[39m func_sig = signature(func)\n\u001b[32m    193\u001b[39m \u001b[38;5;66;03m# Map *args/**kwargs to the function signature\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m194\u001b[39m params = \u001b[43mfunc_sig\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbind\u001b[49m\u001b[43m(\u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    195\u001b[39m params.apply_defaults()\n\u001b[32m    197\u001b[39m \u001b[38;5;66;03m# ignore self/cls and positional/keyword markers\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\inspect.py:3264\u001b[39m, in \u001b[36mSignature.bind\u001b[39m\u001b[34m(self, *args, **kwargs)\u001b[39m\n\u001b[32m   3259\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mbind\u001b[39m(\u001b[38;5;28mself\u001b[39m, /, *args, **kwargs):\n\u001b[32m   3260\u001b[39m \u001b[38;5;250m    \u001b[39m\u001b[33;03m\"\"\"Get a BoundArguments object, that maps the passed `args`\u001b[39;00m\n\u001b[32m   3261\u001b[39m \u001b[33;03m    and `kwargs` to the function's signature.  Raises `TypeError`\u001b[39;00m\n\u001b[32m   3262\u001b[39m \u001b[33;03m    if the passed arguments can not be bound.\u001b[39;00m\n\u001b[32m   3263\u001b[39m \u001b[33;03m    \"\"\"\u001b[39;00m\n\u001b[32m-> \u001b[39m\u001b[32m3264\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43m_bind\u001b[49m\u001b[43m(\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32m~\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\inspect.py:3253\u001b[39m, in \u001b[36mSignature._bind\u001b[39m\u001b[34m(self, args, kwargs, partial)\u001b[39m\n\u001b[32m   3243\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3244\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mgot some positional-only arguments passed as \u001b[39m\u001b[33m'\u001b[39m\n\u001b[32m   3245\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mkeyword arguments: \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   (...)\u001b[39m\u001b[32m   3250\u001b[39m             ),\n\u001b[32m   3251\u001b[39m         )\n\u001b[32m   3252\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m3253\u001b[39m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\n\u001b[32m   3254\u001b[39m             \u001b[33m'\u001b[39m\u001b[33mgot an unexpected keyword argument \u001b[39m\u001b[38;5;132;01m{arg!r}\u001b[39;00m\u001b[33m'\u001b[39m.format(\n\u001b[32m   3255\u001b[39m                 arg=\u001b[38;5;28mnext\u001b[39m(\u001b[38;5;28miter\u001b[39m(kwargs))))\n\u001b[32m   3257\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m._bound_arguments_cls(\u001b[38;5;28mself\u001b[39m, arguments)\n",
      "\u001b[31mTypeError\u001b[39m: got an unexpected keyword argument 'squared'"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.metrics import r2_score, mean_squared_error, mean_absolute_error\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Split already done\n",
    "# Models already trained\n",
    "\n",
    "# --- Predictions ---\n",
    "y_pred_train1 = lr_model1.predict(X1_train)\n",
    "y_pred_test1 = lr_model1.predict(X1_test)\n",
    "\n",
    "y_pred_train2 = lr_model2.predict(X2_train)\n",
    "y_pred_test2 = lr_model2.predict(X2_test)\n",
    "\n",
    "y_pred_train3 = lr_model3.predict(X3_train)\n",
    "y_pred_test3 = lr_model3.predict(X3_test)\n",
    "\n",
    "y_pred_train4 = lr_model4.predict(X4_train)\n",
    "y_pred_test4 = lr_model4.predict(X4_test)\n",
    "\n",
    "# --- Results ---\n",
    "\n",
    "def print_metrics(case_name, y_train, y_pred_train, y_test, y_pred_test):\n",
    "    print(f\"\\n{case_name}\")\n",
    "    print(\"Training R²:\", r2_score(y_train, y_pred_train))\n",
    "    print(\"Test R²:\", r2_score(y_test, y_pred_test))\n",
    "    print(\"Test RMSE:\", mean_squared_error(y_test, y_pred_test, squared=False))\n",
    "    print(\"Test MAE:\", mean_absolute_error(y_test, y_pred_test))\n",
    "\n",
    "print_metrics(\"Case 1: age\", y1_train, y_pred_train1, y1_test, y_pred_test1)\n",
    "print_metrics(\"Case 2: family_size\", y2_train, y_pred_train2, y2_test, y_pred_test2)\n",
    "print_metrics(\"Case 3: age + family_size\", y3_train, y_pred_train3, y3_test, y_pred_test3)\n",
    "print_metrics(\"Case 4: parch\", y4_train, y_pred_train4, y4_test, y_pred_test4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1b27c45",
   "metadata": {},
   "outputs": [],
   "source": [
    "X1_train, X1_test, y1_train, y1_test = train_test_split(X1, y1, test_size=0.2, random_state=123)\n",
    "\n",
    "X2_train, X2_test, y2_train, y2_test = train_test_split(X2, y2, test_size=0.2, random_state=123)\n",
    "\n",
    "X3_train, X3_test, y3_train, y3_test = train_test_split(X3, y3, test_size=0.2, random_state=123)\n",
    "\n",
    "X4_train, X4_test, y4_train, y4_test = train_test_split(X4, y4, test_size=0.2, random_state=123)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ffca86eb",
   "metadata": {},
   "source": [
    "### 4.2 Train and Evaluate Linear Regression Models (all 4 cases)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23199b9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "lr_model1 = LinearRegression().fit(X1_train, y1_train)\n",
    "lr_model2 = LinearRegression().fit(X2_train, y2_train)\n",
    "lr_model3 = LinearRegression().fit(X3_train, y3_train)\n",
    "lr_model4 = LinearRegression().fit(X4_train, y4_train)\n",
    "\n",
    "# Predictions\n",
    "\n",
    "y_pred_train1 = lr_model1.predict(X1_train)\n",
    "y_pred_test1 = lr_model1.predict(X1_test)\n",
    "\n",
    "y_pred_train2 = lr_model2.predict(X2_train)\n",
    "y_pred_test2 = lr_model2.predict(X2_test)\n",
    "\n",
    "# TODO: repeat for case 3 and 4 .... "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "85af3d24",
   "metadata": {},
   "source": [
    "### 4.3 Report Performance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "844440cf",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'y1_pred_train' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[125]\u001b[39m\u001b[32m, line 1\u001b[39m\n\u001b[32m----> \u001b[39m\u001b[32m1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCase 1: Training R²:\u001b[39m\u001b[33m\"\u001b[39m, r2_score(y1_train, \u001b[43my1_pred_train\u001b[49m))\n\u001b[32m      2\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCase 1: Test R²:\u001b[39m\u001b[33m\"\u001b[39m, r2_score(y1_test, y1_pred_test))\n\u001b[32m      3\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mCase 1: Test RMSE:\u001b[39m\u001b[33m\"\u001b[39m, mean_squared_error(y1_test, y1_pred_test, squared=\u001b[38;5;28;01mFalse\u001b[39;00m))\n",
      "\u001b[31mNameError\u001b[39m: name 'y1_pred_train' is not defined"
     ]
    }
   ],
   "source": [
    "print(\"Case 1: Training R²:\", r2_score(y1_train, y1_pred_train))\n",
    "print(\"Case 1: Test R²:\", r2_score(y1_test, y1_pred_test))\n",
    "print(\"Case 1: Test RMSE:\", mean_squared_error(y1_test, y1_pred_test, squared=False))\n",
    "print(\"Case 1: Test MAE:\", mean_absolute_error(y1_test, y1_pred_test))\n",
    "\n",
    "# TODO: Repeat for Cases 2-4...."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Reflection of Section 4:\n",
    "\n",
    "Compare the train vs test results for each.\n",
    "1) Did Case 1 overfit or underfit? Explain:\n",
    "2) Did Case 2 overfit or underfit? Explain:\n",
    "3) Did Case 3 overfit or underfit? Explain:\n",
    "4) Did Case 4 overfit or underfit? Explain:\n",
    "\n",
    "Adding Age\n",
    "1) Did adding age improve the model:\n",
    "2) Propose a possible explanation (consider how age might affect ticket price, and whether the data supports that): \n",
    "\n",
    "Worst\n",
    "1) Which case performed the worst:\n",
    "2) How do you know: \n",
    "3) Do you think adding more training data would improve it (and why/why not): \n",
    "\n",
    "Best\n",
    "1) Which case performed the best:\n",
    "2) How do you know: \n",
    "3) Do you think adding more training data would improve it (and why/why not): \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "282e48ce",
   "metadata": {},
   "source": [
    "### Section 5. Compare Alternative Models\n",
    "In this section, we will take the best-performing case and explore other regression models.\n",
    "\n",
    "Choose Best Case to Continue\n",
    "Choose the best case model from the four cases. Use that model to continue to explore additional continuous prediction models. The following assumes that Case 1 was the best predictor  - this may not be the case. Adjust the code to use your best case model instead. \n",
    "\n",
    "Choosing Options\n",
    "When working with regression models, especially those with multiple input features, we may run into overfitting — where a model fits the training data too closely and performs poorly on new data. To prevent this, we can apply regularization.\n",
    "\n",
    "Regularization adds a penalty to the model’s loss function, discouraging it from using very large weights (coefficients). This makes the model simpler and more likely to generalize well to new data.\n",
    "\n",
    "In general: \n",
    "- If the basic linear regression is overfitting, try Ridge.\n",
    "- If you want the model to automatically select the most important features, try Lasso.\n",
    "- If you want a balanced approach, try Elastic Net."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "322f160c",
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Input X contains NaN.\nRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mValueError\u001b[39m                                Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[103]\u001b[39m\u001b[32m, line 19\u001b[39m\n\u001b[32m     16\u001b[39m results = {}\n\u001b[32m     18\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m name, model \u001b[38;5;129;01min\u001b[39;00m models.items():\n\u001b[32m---> \u001b[39m\u001b[32m19\u001b[39m     \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX_train\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m     20\u001b[39m     y_pred = model.predict(X_test)\n\u001b[32m     21\u001b[39m     mse = mean_squared_error(y_test, y_pred)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\applied-ml-derekfintel\\.venv\\Lib\\site-packages\\sklearn\\base.py:1389\u001b[39m, in \u001b[36m_fit_context.<locals>.decorator.<locals>.wrapper\u001b[39m\u001b[34m(estimator, *args, **kwargs)\u001b[39m\n\u001b[32m   1382\u001b[39m     estimator._validate_params()\n\u001b[32m   1384\u001b[39m \u001b[38;5;28;01mwith\u001b[39;00m config_context(\n\u001b[32m   1385\u001b[39m     skip_parameter_validation=(\n\u001b[32m   1386\u001b[39m         prefer_skip_nested_validation \u001b[38;5;129;01mor\u001b[39;00m global_skip_validation\n\u001b[32m   1387\u001b[39m     )\n\u001b[32m   1388\u001b[39m ):\n\u001b[32m-> \u001b[39m\u001b[32m1389\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfit_method\u001b[49m\u001b[43m(\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\applied-ml-derekfintel\\.venv\\Lib\\site-packages\\sklearn\\linear_model\\_ridge.py:1239\u001b[39m, in \u001b[36mRidge.fit\u001b[39m\u001b[34m(self, X, y, sample_weight)\u001b[39m\n\u001b[32m   1237\u001b[39m _accept_sparse = _get_valid_accept_sparse(sparse.issparse(X), \u001b[38;5;28mself\u001b[39m.solver)\n\u001b[32m   1238\u001b[39m xp, _ = get_namespace(X, y, sample_weight)\n\u001b[32m-> \u001b[39m\u001b[32m1239\u001b[39m X, y = \u001b[43mvalidate_data\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1240\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1241\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1242\u001b[39m \u001b[43m    \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1243\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43m_accept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1244\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43m[\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat64\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m.\u001b[49m\u001b[43mfloat32\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1245\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1246\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmulti_output\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1247\u001b[39m \u001b[43m    \u001b[49m\u001b[43my_numeric\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m   1248\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1249\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28msuper\u001b[39m().fit(X, y, sample_weight=sample_weight)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\applied-ml-derekfintel\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:2961\u001b[39m, in \u001b[36mvalidate_data\u001b[39m\u001b[34m(_estimator, X, y, reset, validate_separately, skip_check_array, **check_params)\u001b[39m\n\u001b[32m   2959\u001b[39m         y = check_array(y, input_name=\u001b[33m\"\u001b[39m\u001b[33my\u001b[39m\u001b[33m\"\u001b[39m, **check_y_params)\n\u001b[32m   2960\u001b[39m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[32m-> \u001b[39m\u001b[32m2961\u001b[39m         X, y = \u001b[43mcheck_X_y\u001b[49m\u001b[43m(\u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43my\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mcheck_params\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   2962\u001b[39m     out = X, y\n\u001b[32m   2964\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m no_val_X \u001b[38;5;129;01mand\u001b[39;00m check_params.get(\u001b[33m\"\u001b[39m\u001b[33mensure_2d\u001b[39m\u001b[33m\"\u001b[39m, \u001b[38;5;28;01mTrue\u001b[39;00m):\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\applied-ml-derekfintel\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1370\u001b[39m, in \u001b[36mcheck_X_y\u001b[39m\u001b[34m(X, y, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_2d, allow_nd, multi_output, ensure_min_samples, ensure_min_features, y_numeric, estimator)\u001b[39m\n\u001b[32m   1364\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1365\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m requires y to be passed, but the target y is None\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1366\u001b[39m     )\n\u001b[32m   1368\u001b[39m ensure_all_finite = _deprecate_force_all_finite(force_all_finite, ensure_all_finite)\n\u001b[32m-> \u001b[39m\u001b[32m1370\u001b[39m X = \u001b[43mcheck_array\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1371\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1372\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1373\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m=\u001b[49m\u001b[43maccept_large_sparse\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1374\u001b[39m \u001b[43m    \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1375\u001b[39m \u001b[43m    \u001b[49m\u001b[43morder\u001b[49m\u001b[43m=\u001b[49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1376\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcopy\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1377\u001b[39m \u001b[43m    \u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m=\u001b[49m\u001b[43mforce_writeable\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1378\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1379\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_2d\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1380\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nd\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1381\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_samples\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1382\u001b[39m \u001b[43m    \u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_min_features\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1383\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1384\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mX\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1385\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1387\u001b[39m y = _check_y(y, multi_output=multi_output, y_numeric=y_numeric, estimator=estimator)\n\u001b[32m   1389\u001b[39m check_consistent_length(X, y)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\applied-ml-derekfintel\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:1107\u001b[39m, in \u001b[36mcheck_array\u001b[39m\u001b[34m(array, accept_sparse, accept_large_sparse, dtype, order, copy, force_writeable, force_all_finite, ensure_all_finite, ensure_non_negative, ensure_2d, allow_nd, ensure_min_samples, ensure_min_features, estimator, input_name)\u001b[39m\n\u001b[32m   1101\u001b[39m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\n\u001b[32m   1102\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33mFound array with dim \u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[33m. \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[33m expected <= 2.\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   1103\u001b[39m         % (array.ndim, estimator_name)\n\u001b[32m   1104\u001b[39m     )\n\u001b[32m   1106\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m ensure_all_finite:\n\u001b[32m-> \u001b[39m\u001b[32m1107\u001b[39m     \u001b[43m_assert_all_finite\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m   1108\u001b[39m \u001b[43m        \u001b[49m\u001b[43marray\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1109\u001b[39m \u001b[43m        \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1110\u001b[39m \u001b[43m        \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m   1111\u001b[39m \u001b[43m        \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mensure_all_finite\u001b[49m\u001b[43m \u001b[49m\u001b[43m==\u001b[49m\u001b[43m \u001b[49m\u001b[33;43m\"\u001b[39;49m\u001b[33;43mallow-nan\u001b[39;49m\u001b[33;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[32m   1112\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m   1114\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m copy:\n\u001b[32m   1115\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m _is_numpy_namespace(xp):\n\u001b[32m   1116\u001b[39m         \u001b[38;5;66;03m# only make a copy if `array` and `array_orig` may share memory`\u001b[39;00m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\applied-ml-derekfintel\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:120\u001b[39m, in \u001b[36m_assert_all_finite\u001b[39m\u001b[34m(X, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    117\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m first_pass_isfinite:\n\u001b[32m    118\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m120\u001b[39m \u001b[43m_assert_all_finite_element_wise\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    121\u001b[39m \u001b[43m    \u001b[49m\u001b[43mX\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    122\u001b[39m \u001b[43m    \u001b[49m\u001b[43mxp\u001b[49m\u001b[43m=\u001b[49m\u001b[43mxp\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    123\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m=\u001b[49m\u001b[43mallow_nan\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    124\u001b[39m \u001b[43m    \u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m=\u001b[49m\u001b[43mmsg_dtype\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    125\u001b[39m \u001b[43m    \u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mestimator_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    126\u001b[39m \u001b[43m    \u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43minput_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    127\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Projects\\applied-ml-derekfintel\\.venv\\Lib\\site-packages\\sklearn\\utils\\validation.py:169\u001b[39m, in \u001b[36m_assert_all_finite_element_wise\u001b[39m\u001b[34m(X, xp, allow_nan, msg_dtype, estimator_name, input_name)\u001b[39m\n\u001b[32m    152\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m estimator_name \u001b[38;5;129;01mand\u001b[39;00m input_name == \u001b[33m\"\u001b[39m\u001b[33mX\u001b[39m\u001b[33m\"\u001b[39m \u001b[38;5;129;01mand\u001b[39;00m has_nan_error:\n\u001b[32m    153\u001b[39m     \u001b[38;5;66;03m# Improve the error message on how to handle missing values in\u001b[39;00m\n\u001b[32m    154\u001b[39m     \u001b[38;5;66;03m# scikit-learn.\u001b[39;00m\n\u001b[32m    155\u001b[39m     msg_err += (\n\u001b[32m    156\u001b[39m         \u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mestimator_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m does not accept missing values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    157\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m encoded as NaN natively. For supervised learning, you might want\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m   (...)\u001b[39m\u001b[32m    167\u001b[39m         \u001b[33m\"\u001b[39m\u001b[33m#estimators-that-handle-nan-values\u001b[39m\u001b[33m\"\u001b[39m\n\u001b[32m    168\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m169\u001b[39m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(msg_err)\n",
      "\u001b[31mValueError\u001b[39m: Input X contains NaN.\nRidge does not accept missing values encoded as NaN natively. For supervised learning, you might want to consider sklearn.ensemble.HistGradientBoostingClassifier and Regressor which accept missing values encoded as NaNs natively. Alternatively, it is possible to preprocess the data, for instance by using an imputer transformer in a pipeline or drop samples with missing values. See https://scikit-learn.org/stable/modules/impute.html You can find a list of all estimators that handle NaN values at the following page: https://scikit-learn.org/stable/modules/impute.html#estimators-that-handle-nan-values"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import Ridge, Lasso, ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "import numpy as np\n",
    "\n",
    "# Assuming X and y are your features and target from the best case\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Dictionary to store models and results\n",
    "models = {\n",
    "    \"Ridge\": Ridge(alpha=1.0),\n",
    "    \"Lasso\": Lasso(alpha=0.1),\n",
    "    \"ElasticNet\": ElasticNet(alpha=0.1, l1_ratio=0.5)\n",
    "}\n",
    "\n",
    "results = {}\n",
    "\n",
    "for name, model in models.items():\n",
    "    model.fit(X_train, y_train)\n",
    "    y_pred = model.predict(X_test)\n",
    "    mse = mean_squared_error(y_test, y_pred)\n",
    "    r2 = r2_score(y_test, y_pred)\n",
    "    results[name] = {\n",
    "        \"MSE\": mse,\n",
    "        \"R²\": r2,\n",
    "        \"Coefficients\": model.coef_\n",
    "    }\n",
    "    print(f\"{name} Regression - MSE: {mse:.3f}, R²: {r2:.3f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14fb5dad",
   "metadata": {},
   "source": [
    "### 5.2 Elastic Net (L1 + L2 combined)\n",
    "Lasso Regression uses the L1 penalty, which adds the sum of absolute values of the coefficients to the loss function. Lasso can shrink some coefficients all the way to zero, effectively removing less important features. This makes it useful for feature selection.\n",
    "\n",
    "- Penalty term: L1 = sum of absolute values of weights\n",
    "- Effect: Can shrink some weights to zero (drops features), simplifies the model\n",
    "Elastic Net combines both L1 (Lasso) and L2 (Ridge) penalties. It balances the feature selection ability of Lasso with the stability of Ridge.\n",
    "\n",
    "We control the balance with a parameter called l1_ratio:\n",
    "- If l1_ratio = 0, it behaves like Ridge\n",
    "- If l1_ratio = 1, it behaves like Lasso\n",
    "- Values in between mix both types\n",
    "- Penalty term: α × (L1 + L2)\n",
    "- Effect: Shrinks weights and can drop some features — flexible and powerful"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18e55163",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Elastic Net - MSE: 1441.761, R²: 0.003\n",
      "Elastic Net Coefficients: [0.40684608]\n"
     ]
    }
   ],
   "source": [
    "from sklearn.linear_model import ElasticNet\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Create Elastic Net model with a balance between L1 and L2\n",
    "elastic_model = ElasticNet(alpha=0.3, l1_ratio=0.5, random_state=42)\n",
    "elastic_model.fit(X1_train, y1_train)\n",
    "\n",
    "# Predict on test set\n",
    "y_pred_elastic = elastic_model.predict(X1_test)\n",
    "\n",
    "# Evaluate\n",
    "mse_elastic = mean_squared_error(y1_test, y_pred_elastic)\n",
    "r2_elastic = r2_score(y1_test, y_pred_elastic)\n",
    "\n",
    "print(f\"Elastic Net - MSE: {mse_elastic:.3f}, R²: {r2_elastic:.3f}\")\n",
    "print(f\"Elastic Net Coefficients: {elastic_model.coef_}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "84943b72",
   "metadata": {},
   "outputs": [],
   "source": [
    "#elastic_model = ElasticNet(alpha=0.3, l1_ratio=0.5)\n",
    "#elastic_model.fit(X1_train, y1_train)\n",
    "#y_pred_elastic = elastic_model.predict(X1_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d3e9eae3",
   "metadata": {},
   "source": [
    "### 5.3 Polynomial Regression\n",
    "Linear regression is a simple two dimensional relationship - a simple straight line. But we can test more complex relationships. Polynomial regression adds interaction and nonlinear terms to the model. Be careful here - higher-degree polynomials can easily overfit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afce0b33",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X1_test_poly' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[102]\u001b[39m\u001b[32m, line 9\u001b[39m\n\u001b[32m      7\u001b[39m poly_model = LinearRegression()\n\u001b[32m      8\u001b[39m poly_model.fit(X_train_poly, y1_train)\n\u001b[32m----> \u001b[39m\u001b[32m9\u001b[39m y_pred_poly = poly_model.predict(\u001b[43mX1_test_poly\u001b[49m)\n",
      "\u001b[31mNameError\u001b[39m: name 'X1_test_poly' is not defined"
     ]
    }
   ],
   "source": [
    "# Set up the poly inputs\n",
    "poly = PolynomialFeatures(degree=3)\n",
    "X_train_poly = poly.fit_transform(X1_train)\n",
    "X_test_poly = poly.transform(X1_test)\n",
    "\n",
    "# Use the poly inputs in the LR model\n",
    "poly_model = LinearRegression()\n",
    "poly_model.fit(X_train_poly, y1_train)\n",
    "y_pred_poly = poly_model.predict(X1_test_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1be2652",
   "metadata": {},
   "source": [
    "### 5.4 Visualize Polynomial Cubic Fit (for 1 input feature)\n",
    "Choose a case with just one input feature and plot it. For example:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9beafc6",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.scatter(X1_test, y1_test, color='blue', label='Actual')\n",
    "plt.scatter(X1_test, y_pred_poly, color='red', label='Predicted (Poly)')\n",
    "plt.legend()\n",
    "plt.title(\"Polynomial Regression: Age vs Fare\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d78a6aac",
   "metadata": {},
   "source": [
    "### 5.4 Reflections:\n",
    "\n",
    "1) What patterns does the cubic model seem to capture:\n",
    "2) Where does it perform well or poorly:\n",
    "3) Did the polynomial fit outperform linear regression:\n",
    "4) Where (on the graph or among which kinds of data points) does it fit best:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03dca114",
   "metadata": {},
   "source": [
    "### 5.4 Compare All Models\n",
    "Create a summary table or printout comparing all models:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "14f504de",
   "metadata": {},
   "outputs": [],
   "source": [
    "def report(name, y_true, y_pred):\n",
    "    print(f\"{name} R²: {r2_score(y_true, y_pred):.3f}\")\n",
    "    print(f\"{name} RMSE: {mean_squared_error(y_true, y_pred, squared=False):.2f}\")\n",
    "    print(f\"{name} MAE: {mean_absolute_error(y_true, y_pred):.2f}\\n\")\n",
    "\n",
    "report(\"Linear\", y1_test, y1_pred_test)\n",
    "report(\"Ridge\", y1_test, y_pred_ridge)\n",
    "report(\"ElasticNet\", y1_test, y_pred_elastic)\n",
    "report(\"Polynomial\", y1_test, y_pred_poly)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "341db63d",
   "metadata": {},
   "source": [
    "### 5.5 Visualize Higher Order Polynomial (for the same 1 input case)\n",
    "Use the same single input case as you visualized above, but use a higher degree polynomial (e.g. 4, 5, 6, 7, or 8). Plot the result. \n",
    "\n",
    "In a Markdown cell, tell us which option seems to work better - your initial cubic (3) or your higher order and why. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1da5716d",
   "metadata": {},
   "source": [
    "### Section 6. Final Thoughts & Insights\n",
    "Your notebook should tell a data story. Use this section to demonstrate your thinking and value as an analyst.\n",
    "\n",
    "### 6.1 Summarize Findings\n",
    "1) What features were most useful?\n",
    "2) What regression model performed best?\n",
    "3) How did model complexity or regularization affect results?\n",
    "\n",
    "### 6.2 Discuss Challenges\n",
    "1) Was fare hard to predict? Why?\n",
    "2) Did skew or outliers impact the models?\n",
    "\n",
    "### 6.3 Optional Next Steps\n",
    "1) Try different features besides the ones used (e.g., pclass, sex if you didn't use them this time)\n",
    "2) Try predicting age instead of fare\n",
    "3) Explore log transformation of fare to reduce skew"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
